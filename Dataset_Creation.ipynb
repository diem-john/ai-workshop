{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcb3f78-d953-43d5-8543-7ab0dfa4b477",
   "metadata": {},
   "source": [
    "# Time Series Data Preprocessing and Model Preparation\n",
    "\n",
    "In this notebook, we will:\n",
    "- Normalize the `Global_active_power` feature using `MinMaxScaler`.\n",
    "- Split the dataset into training, validation, and test sets.\n",
    "- Create a function to generate input (`X`) and output (`y`) sets for time series forecasting.\n",
    "- Output the shapes of the training, validation and test sets.\n",
    "\n",
    "1. [Imports and Setup](#1.-Imports-and-Setup)\n",
    "2. [Data Normalization](#2.-Data-Normalization)\n",
    "3. [Define Time Periods for Training and Validation Sets](#3.-Define-Time-Periods-for-Training-and-Validation-Sets)\n",
    "4. [Create X and y Sets](#4.-Create-X-and-y-Sets)\n",
    "5. [Data Saving and Loading](#5.-Data-Saving-and-Loading)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e756770-26b1-406b-868b-aa92afd4ac66",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "We will use the following libraries:\n",
    "- `pandas` for data manipulation.\n",
    "- `numpy` for handling arrays.\n",
    "- `MinMaxScaler` from `sklearn.preprocessing` to normalize the data.\n",
    "- `joblib` to save and load the scaler for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48352adc-08d5-4cdb-9fb8-c1355fe2b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b552bb9-385c-40d8-86e3-cad8c0f49ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv('global_active_power_hourly.csv', index_col='Date',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5220c53f-2ae3-426d-be6e-2452d08169f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_active_power</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:00:00</th>\n",
       "      <td>0.636816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 18:00:00</th>\n",
       "      <td>0.545045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 19:00:00</th>\n",
       "      <td>0.509006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 20:00:00</th>\n",
       "      <td>0.488550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 21:00:00</th>\n",
       "      <td>0.455597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-26 17:00:00</th>\n",
       "      <td>0.248876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-26 18:00:00</th>\n",
       "      <td>0.225194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-26 19:00:00</th>\n",
       "      <td>0.238534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-26 20:00:00</th>\n",
       "      <td>0.161531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-26 21:00:00</th>\n",
       "      <td>0.125948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34589 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Global_active_power\n",
       "Date                                    \n",
       "2006-12-16 17:00:00             0.636816\n",
       "2006-12-16 18:00:00             0.545045\n",
       "2006-12-16 19:00:00             0.509006\n",
       "2006-12-16 20:00:00             0.488550\n",
       "2006-12-16 21:00:00             0.455597\n",
       "...                                  ...\n",
       "2010-11-26 17:00:00             0.248876\n",
       "2010-11-26 18:00:00             0.225194\n",
       "2010-11-26 19:00:00             0.238534\n",
       "2010-11-26 20:00:00             0.161531\n",
       "2010-11-26 21:00:00             0.125948\n",
       "\n",
       "[34589 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8694b89-ae13-4bbb-9150-b5e15c6e647d",
   "metadata": {},
   "source": [
    "## 2. Data Normalization\n",
    "Normalizing the 'Global_active_power' column ensures that the data is on the same scale, which can improve the performance of many machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d20df1-27af-4497-a42a-abca286f52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df['Global_active_power'] = scaler.fit_transform(df[['Global_active_power']])\n",
    "\n",
    "# Save the scaler for inverse transformation\n",
    "# Saving the scaler allows us to revert the normalization when needed (e.g., to interpret predictions in original units).\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a0ac7-7891-47a8-8d0d-9676bdd82645",
   "metadata": {},
   "source": [
    "## 3. Define Time Periods for Training and Validation Sets\n",
    "- Setting the periods for training, validation, and testing helps to create a systematic approach for model evaluation.\n",
    "- Creating the training and validation datasets from the defined periods. An 80-20 split is used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ee6be-1596-4988-afbe-2f7312bd61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_start = '2006-12-16' # 2006 to 2008\n",
    "train_val_end = '2008-12-31'\n",
    "test_start_1 = '2009-01-01' # 2009\n",
    "test_end_1 = '2009-12-31'\n",
    "test_start_2 = '2010-01-01' # 2010\n",
    "test_end_2 = '2010-12-31'\n",
    "\n",
    "# Spliting Training and validation (80-20 split)\n",
    "train_val_df = df[train_val_start:train_val_end]\n",
    "split_index = int(len(train_val_df) * 0.8)\n",
    "train_df = train_val_df[:split_index] # 80%\n",
    "val_df = train_val_df[split_index:] # 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99221dd-f174-4cda-9c77-86125af6efb1",
   "metadata": {},
   "source": [
    "## 4. Create X and y Sets\n",
    "\n",
    "Prepares the time series data for forecasting by creating input (X) and output (y) sets. This process is used for training machine learning models, as it transforms historical data into a format that the models can learn from. Defining a look-back period and a prediction horizon enables the model to capture temporal patterns and make informed predictions about future values. This preparation step lays the foundation for accurate forecasting and effective model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf3469-41f7-4130-ba19-80c4deaafdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shapes - X: (14303, 24) , y: (14303, 2)\n",
      "Validation set shapes - X: (3558, 24) , y: (3558, 2)\n",
      "Test set 1 shapes - X: (8735, 24) , y: (8735, 2)\n",
      "Test set 2 shapes - X: (7893, 24) , y: (7893, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_X_y(data, look_back, prediction_horizon):\n",
    "    \"\"\"\n",
    "    Create input (X) and output (y) sets for time series forecasting.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.Series): The time series data from which to create X and y sets.\n",
    "    look_back (int): The number of previous time steps to use as input features.\n",
    "    prediction_horizon (int): The number of time steps to predict into the future.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays (X, y) where:\n",
    "        - X contains the input features for the model\n",
    "        - y contains the corresponding output values to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize empty lists to hold the input (X) and output (y) sets\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Loop through the data, starting from the look_back period to the end\n",
    "    for i in range(look_back, len(data) - prediction_horizon + 1):\n",
    "        # Append the last 'look_back' values to X\n",
    "        X.append(data.iloc[i - look_back:i].values)\n",
    "        \n",
    "        # Append the next 'prediction_horizon' values to y\n",
    "        y.append(data.iloc[i:i + prediction_horizon].values)\n",
    "    \n",
    "    # Convert the lists to numpy arrays for model compatibility\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define Parameters for X and y Creation\n",
    "look_back = 24  # Look back period in hours\n",
    "prediction_horizon = 2  # Predict the next 2 hours\n",
    "\n",
    "# Now we will create the X and y sets for each dataset (training, validation, and testing).\n",
    "\n",
    "# Training Set\n",
    "X_train, y_train = create_X_y(train_df['Global_active_power'], look_back, prediction_horizon)\n",
    "\n",
    "# Validation Set\n",
    "X_val, y_val = create_X_y(val_df['Global_active_power'], look_back, prediction_horizon)\n",
    "\n",
    "# Test Set 1\n",
    "test_data_1 = df[test_start_1:test_end_1]['Global_active_power']\n",
    "X_test_1, y_test_1 = create_X_y(test_data_1, look_back, prediction_horizon)\n",
    "\n",
    "# Test Set 2\n",
    "test_data_2 = df[test_start_2:test_end_2]['Global_active_power']\n",
    "X_test_2, y_test_2 = create_X_y(test_data_2, look_back, prediction_horizon)\n",
    "\n",
    "# Print the shapes of the generated datasets to verify that they have been created correctly.\n",
    "print(\"Train set shapes - X:\", X_train.shape, \", y:\", y_train.shape)\n",
    "print(\"Validation set shapes - X:\", X_val.shape, \", y:\", y_val.shape)\n",
    "print(\"Test set 1 shapes - X:\", X_test_1.shape, \", y:\", y_test_1.shape)\n",
    "print(\"Test set 2 shapes - X:\", X_test_2.shape, \", y:\", y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03b22-565f-4a02-b6a7-7be4139668b1",
   "metadata": {},
   "source": [
    "## 5. Data Saving and Loading\n",
    "Saving the preprocessed training, validation, and test datasets to a compressed `.npz` file for efficient storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a42c993-0180-4711-a04a-69591d99d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Train set shapes - X: (14303, 24) , y: (14303, 2)\n",
      "Loaded Validation set shapes - X: (3558, 24) , y: (3558, 2)\n",
      "Loaded Test set 1 shapes - X: (8735, 24) , y: (8735, 2)\n",
      "Loaded Test set 2 shapes - X: (7893, 24) , y: (7893, 2)\n"
     ]
    }
   ],
   "source": [
    "# np.savez('power_data.npz', \n",
    "#          X_train=X_train, y_train=y_train,\n",
    "#          X_val=X_val, y_val=y_val,\n",
    "#          X_test_1=X_test_1, y_test_1=y_test_1,\n",
    "#          X_test_2=X_test_2, y_test_2=y_test_2)\n",
    "\n",
    "\n",
    "# Load the datasets from the .npz file\n",
    "loaded_data = np.load('power_data.npz')\n",
    "\n",
    "# Accessing the loaded data\n",
    "X_train_loaded = loaded_data['X_train']\n",
    "y_train_loaded = loaded_data['y_train']\n",
    "X_val_loaded = loaded_data['X_val']\n",
    "y_val_loaded = loaded_data['y_val']\n",
    "X_test_1_loaded = loaded_data['X_test_1']\n",
    "y_test_1_loaded = loaded_data['y_test_1']\n",
    "X_test_2_loaded = loaded_data['X_test_2']\n",
    "y_test_2_loaded = loaded_data['y_test_2']\n",
    "\n",
    "# Output the shapes of the loaded sets\n",
    "print(\"Loaded Train set shapes - X:\", X_train_loaded.shape, \", y:\", y_train_loaded.shape)\n",
    "print(\"Loaded Validation set shapes - X:\", X_val_loaded.shape, \", y:\", y_val_loaded.shape)\n",
    "print(\"Loaded Test set 1 shapes - X:\", X_test_1_loaded.shape, \", y:\", y_test_1_loaded.shape)\n",
    "print(\"Loaded Test set 2 shapes - X:\", X_test_2_loaded.shape, \", y:\", y_test_2_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c3e45-4a55-42ae-a0a8-3902e23a08d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
